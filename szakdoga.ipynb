{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import to_datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "# load the train data\n",
    "dataframe = read_csv('dataset/train.csv',usecols=[i for i in range(0,211)], engine='python', header=0)\n",
    "trainData = dataframe.values\n",
    "trainDataOriginal = np.nan_to_num(trainData)\n",
    "trainData = trainDataOriginal\n",
    "# load the test data\n",
    "dataframe = read_csv('dataset/test_2.csv',usecols=[i for i in range(0,146)], engine='python', header=0)\n",
    "testData = dataframe.values\n",
    "testData = np.nan_to_num(testData)\n",
    "testX = testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "batch_size = 64\n",
    "input_dim  = 146\n",
    "output_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(625, 64, 146)\n",
      "(625, 64, 64)\n",
      "(1875, 64, 146)\n"
     ]
    }
   ],
   "source": [
    "#Separate the train data to input and output\n",
    "trainX = trainData[0:len(trainData),0:146]\n",
    "trainY = trainData[0:len(trainData),147:211]\n",
    "# normalize the dataset\n",
    "scalerInput = MinMaxScaler(feature_range=(0, 1))\n",
    "scalerOutput = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "trainX = scalerInput.fit_transform(trainX)\n",
    "trainY = scalerOutput.fit_transform(trainY)\n",
    "testX = scalerInput.fit_transform(testX)\n",
    "\n",
    "#Put it into batches\n",
    "trainX = np.reshape(trainX,(len(trainX)/batch_size,batch_size,trainX.shape[1]))\n",
    "trainY = np.reshape(trainY,(len(trainY)/batch_size,batch_size,trainY.shape[1]))\n",
    "testX  = np.reshape(testX,(len(testX)/batch_size,batch_size,testX.shape[1]))\n",
    "\n",
    "print trainX.shape\n",
    "print trainY.shape\n",
    "print testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.activations import linear\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Model\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, trainX, trainY):\n",
    "        self.optimizer = adam(0.01, 0.01, 0.001, 1e-08, 0.0)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(output_dim, input_shape=(batch_size, input_dim), return_sequences=True))\n",
    "        self.model.add(Dense(output_dim, activation='linear'))\n",
    "        #Configures the model for training.\n",
    "        self.model.compile(loss=\"mean_squared_error\",optimizer=self.optimizer)\n",
    "        #Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "        self.model.fit(trainX,trainY,batch_size=batch_size,epochs=100)\n",
    "        #Make predictions\n",
    "    \n",
    "    def predict(self,testX):\n",
    "        return self.model.predict(testX)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "625/625 [==============================] - 2s - loss: 0.0938     \n",
      "Epoch 2/100\n",
      "625/625 [==============================] - 1s - loss: 0.0271     \n",
      "Epoch 3/100\n",
      "625/625 [==============================] - 1s - loss: 0.0234     \n",
      "Epoch 4/100\n",
      "625/625 [==============================] - 1s - loss: 0.0218     \n",
      "Epoch 5/100\n",
      "625/625 [==============================] - 1s - loss: 0.0214     \n",
      "Epoch 6/100\n",
      "625/625 [==============================] - 1s - loss: 0.0200     \n",
      "Epoch 7/100\n",
      "625/625 [==============================] - 2s - loss: 0.0191     \n",
      "Epoch 8/100\n",
      "625/625 [==============================] - 1s - loss: 0.0181     \n",
      "Epoch 9/100\n",
      "625/625 [==============================] - 1s - loss: 0.0184     \n",
      "Epoch 10/100\n",
      "625/625 [==============================] - 1s - loss: 0.0217     \n",
      "Epoch 11/100\n",
      "625/625 [==============================] - 2s - loss: 0.0203     \n",
      "Epoch 12/100\n",
      "625/625 [==============================] - 2s - loss: 0.0196     \n",
      "Epoch 13/100\n",
      "625/625 [==============================] - 1s - loss: 0.0191     \n",
      "Epoch 14/100\n",
      "625/625 [==============================] - 1s - loss: 0.0188     \n",
      "Epoch 15/100\n",
      "625/625 [==============================] - 1s - loss: 0.0184     \n",
      "Epoch 16/100\n",
      "625/625 [==============================] - 2s - loss: 0.0180     - ETA: 0s - loss: 0.\n",
      "Epoch 17/100\n",
      "625/625 [==============================] - 1s - loss: 0.0177     \n",
      "Epoch 18/100\n",
      "625/625 [==============================] - 1s - loss: 0.0177     \n",
      "Epoch 19/100\n",
      "625/625 [==============================] - 1s - loss: 0.0174     \n",
      "Epoch 20/100\n",
      "625/625 [==============================] - 1s - loss: 0.0172     \n",
      "Epoch 21/100\n",
      "625/625 [==============================] - 2s - loss: 0.0171     \n",
      "Epoch 22/100\n",
      "625/625 [==============================] - 1s - loss: 0.0169     - ETA: 0s - loss: 0.017\n",
      "Epoch 23/100\n",
      "625/625 [==============================] - 1s - loss: 0.0167     \n",
      "Epoch 24/100\n",
      "625/625 [==============================] - 1s - loss: 0.0166     \n",
      "Epoch 25/100\n",
      "625/625 [==============================] - 1s - loss: 0.0164     \n",
      "Epoch 26/100\n",
      "625/625 [==============================] - 2s - loss: 0.0162     \n",
      "Epoch 27/100\n",
      "625/625 [==============================] - 2s - loss: 0.0162     \n",
      "Epoch 28/100\n",
      "625/625 [==============================] - 2s - loss: 0.0161     \n",
      "Epoch 29/100\n",
      "625/625 [==============================] - 1s - loss: 0.0159     \n",
      "Epoch 30/100\n",
      "625/625 [==============================] - 2s - loss: 0.0158     \n",
      "Epoch 31/100\n",
      "625/625 [==============================] - 2s - loss: 0.0157     \n",
      "Epoch 32/100\n",
      "625/625 [==============================] - 1s - loss: 0.0157     \n",
      "Epoch 33/100\n",
      "625/625 [==============================] - 1s - loss: 0.0155     \n",
      "Epoch 34/100\n",
      "625/625 [==============================] - 2s - loss: 0.0154     \n",
      "Epoch 35/100\n",
      "625/625 [==============================] - 2s - loss: 0.0153     \n",
      "Epoch 36/100\n",
      "625/625 [==============================] - 1s - loss: 0.0152     \n",
      "Epoch 37/100\n",
      "625/625 [==============================] - 1s - loss: 0.0151     \n",
      "Epoch 38/100\n",
      "625/625 [==============================] - 2s - loss: 0.0150     \n",
      "Epoch 39/100\n",
      "625/625 [==============================] - 2s - loss: 0.0150     \n",
      "Epoch 40/100\n",
      "625/625 [==============================] - 2s - loss: 0.0149     \n",
      "Epoch 41/100\n",
      "625/625 [==============================] - 2s - loss: 0.0148     \n",
      "Epoch 42/100\n",
      "625/625 [==============================] - 2s - loss: 0.0147     \n",
      "Epoch 43/100\n",
      "625/625 [==============================] - 2s - loss: 0.0146     \n",
      "Epoch 44/100\n",
      "625/625 [==============================] - 1s - loss: 0.0145     \n",
      "Epoch 45/100\n",
      "625/625 [==============================] - 2s - loss: 0.0145     \n",
      "Epoch 46/100\n",
      "625/625 [==============================] - 2s - loss: 0.0144     \n",
      "Epoch 47/100\n",
      "625/625 [==============================] - 2s - loss: 0.0143     \n",
      "Epoch 48/100\n",
      "625/625 [==============================] - 1s - loss: 0.0142     \n",
      "Epoch 49/100\n",
      "625/625 [==============================] - 2s - loss: 0.0141     \n",
      "Epoch 50/100\n",
      "625/625 [==============================] - 2s - loss: 0.0140     \n",
      "Epoch 51/100\n",
      "625/625 [==============================] - 2s - loss: 0.0139     \n",
      "Epoch 52/100\n",
      "625/625 [==============================] - 3s - loss: 0.0138     \n",
      "Epoch 53/100\n",
      "625/625 [==============================] - 3s - loss: 0.0137     \n",
      "Epoch 54/100\n",
      "625/625 [==============================] - 3s - loss: 0.0136     \n",
      "Epoch 55/100\n",
      "625/625 [==============================] - 3s - loss: 0.0137     \n",
      "Epoch 56/100\n",
      "625/625 [==============================] - 2s - loss: 0.0136     \n",
      "Epoch 57/100\n",
      "625/625 [==============================] - 2s - loss: 0.0135     \n",
      "Epoch 58/100\n",
      "625/625 [==============================] - 2s - loss: 0.0134     \n",
      "Epoch 59/100\n",
      "625/625 [==============================] - 2s - loss: 0.0133     \n",
      "Epoch 60/100\n",
      "625/625 [==============================] - 2s - loss: 0.0132     \n",
      "Epoch 61/100\n",
      "625/625 [==============================] - 2s - loss: 0.0135     \n",
      "Epoch 62/100\n",
      "625/625 [==============================] - 2s - loss: 0.0136     \n",
      "Epoch 63/100\n",
      "625/625 [==============================] - 1s - loss: 0.0133     \n",
      "Epoch 64/100\n",
      "625/625 [==============================] - 2s - loss: 0.0132     \n",
      "Epoch 65/100\n",
      "625/625 [==============================] - 2s - loss: 0.0132     \n",
      "Epoch 66/100\n",
      "625/625 [==============================] - 2s - loss: 0.0132     \n",
      "Epoch 67/100\n",
      "625/625 [==============================] - 1s - loss: 0.0131     \n",
      "Epoch 68/100\n",
      "625/625 [==============================] - 2s - loss: 0.0130     \n",
      "Epoch 69/100\n",
      "625/625 [==============================] - 2s - loss: 0.0129     \n",
      "Epoch 70/100\n",
      "625/625 [==============================] - 2s - loss: 0.0129     \n",
      "Epoch 71/100\n",
      "625/625 [==============================] - 1s - loss: 0.0128     \n",
      "Epoch 72/100\n",
      "625/625 [==============================] - 2s - loss: 0.0127     \n",
      "Epoch 73/100\n",
      "625/625 [==============================] - 2s - loss: 0.0126     \n",
      "Epoch 74/100\n",
      "625/625 [==============================] - 2s - loss: 0.0126     \n",
      "Epoch 75/100\n",
      "625/625 [==============================] - 2s - loss: 0.0125     \n",
      "Epoch 76/100\n",
      "625/625 [==============================] - 2s - loss: 0.0124     \n",
      "Epoch 77/100\n",
      "625/625 [==============================] - 2s - loss: 0.0125     \n",
      "Epoch 78/100\n",
      "625/625 [==============================] - 2s - loss: 0.0125     \n",
      "Epoch 79/100\n",
      "625/625 [==============================] - 2s - loss: 0.0124     \n",
      "Epoch 80/100\n",
      "625/625 [==============================] - 2s - loss: 0.0123     \n",
      "Epoch 81/100\n",
      "625/625 [==============================] - 1s - loss: 0.0121     \n",
      "Epoch 82/100\n",
      "625/625 [==============================] - 2s - loss: 0.0124     \n",
      "Epoch 83/100\n",
      "625/625 [==============================] - 2s - loss: 0.0123     \n",
      "Epoch 84/100\n",
      "625/625 [==============================] - 1s - loss: 0.0122     \n",
      "Epoch 85/100\n",
      "625/625 [==============================] - 1s - loss: 0.0121     \n",
      "Epoch 86/100\n",
      "625/625 [==============================] - 2s - loss: 0.0120     \n",
      "Epoch 87/100\n",
      "625/625 [==============================] - 2s - loss: 0.0119     \n",
      "Epoch 88/100\n",
      "625/625 [==============================] - 1s - loss: 0.0118     \n",
      "Epoch 89/100\n",
      "625/625 [==============================] - 2s - loss: 0.0117     \n",
      "Epoch 90/100\n",
      "625/625 [==============================] - 2s - loss: 0.0116     \n",
      "Epoch 91/100\n",
      "625/625 [==============================] - 2s - loss: 0.0115     \n",
      "Epoch 92/100\n",
      "625/625 [==============================] - 1s - loss: 0.0119     \n",
      "Epoch 93/100\n",
      "625/625 [==============================] - 2s - loss: 0.0120     \n",
      "Epoch 94/100\n",
      "625/625 [==============================] - 2s - loss: 0.0118     \n",
      "Epoch 95/100\n",
      "625/625 [==============================] - 2s - loss: 0.0115     \n",
      "Epoch 96/100\n",
      "625/625 [==============================] - 2s - loss: 0.0112     \n",
      "Epoch 97/100\n",
      "625/625 [==============================] - 2s - loss: 0.0109     \n",
      "Epoch 98/100\n",
      "625/625 [==============================] - 2s - loss: 0.0114     \n",
      "Epoch 99/100\n",
      "625/625 [==============================] - 2s - loss: 0.0113     \n",
      "Epoch 100/100\n",
      "625/625 [==============================] - 2s - loss: 0.0114     \n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "nn = Model(trainX, trainY)\n",
    "testY = nn.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.27722538  0.41191441  0.25778103 ...,  0.39068079  0.22185877\n",
      "    0.18526031]\n",
      "  [ 0.30170268  0.4142175   0.21979235 ...,  0.3276042   0.16439402\n",
      "    0.1669545 ]\n",
      "  [ 0.28268045  0.42811126  0.23159155 ...,  0.35434252  0.1559689\n",
      "    0.17481042]\n",
      "  ..., \n",
      "  [ 0.28803959  0.42057824  0.21537572 ...,  0.33112931  0.17314607\n",
      "    0.17330882]\n",
      "  [ 0.28497991  0.41929254  0.2179479  ...,  0.32386723  0.15691514\n",
      "    0.17446931]\n",
      "  [ 0.28286085  0.42736083  0.21171799 ...,  0.3156395   0.14905316\n",
      "    0.17400275]]\n",
      "\n",
      " [[ 0.26867196  0.41901064  0.25016084 ...,  0.3912757   0.2162506\n",
      "    0.17333689]\n",
      "  [ 0.29639587  0.41994682  0.22336958 ...,  0.35169125  0.17085397\n",
      "    0.17246018]\n",
      "  [ 0.28630573  0.43777227  0.22766322 ...,  0.34057897  0.16546756\n",
      "    0.17940933]\n",
      "  ..., \n",
      "  [ 0.28171715  0.41220164  0.22205271 ...,  0.34943539  0.17107639\n",
      "    0.17578305]\n",
      "  [ 0.29075569  0.41770026  0.22600949 ...,  0.34467626  0.16993889\n",
      "    0.18187602]\n",
      "  [ 0.2763671   0.43220574  0.22291861 ...,  0.34962893  0.14606968\n",
      "    0.17604306]]\n",
      "\n",
      " [[ 0.26485199  0.40478685  0.24238986 ...,  0.39538264  0.22364315\n",
      "    0.18143281]\n",
      "  [ 0.28471866  0.41109112  0.2043795  ...,  0.31299046  0.15225399\n",
      "    0.15226306]\n",
      "  [ 0.29646972  0.41424391  0.23709576 ...,  0.34243393  0.17140585\n",
      "    0.18565863]\n",
      "  ..., \n",
      "  [ 0.28391105  0.42793009  0.22472617 ...,  0.33807415  0.15713334\n",
      "    0.17131735]\n",
      "  [ 0.28547788  0.41033807  0.21871153 ...,  0.35062349  0.1728597\n",
      "    0.17545182]\n",
      "  [ 0.2859095   0.41700166  0.226944   ...,  0.35403877  0.16743195\n",
      "    0.17629315]]\n",
      "\n",
      " ..., \n",
      " [[ 0.26224196  0.42063352  0.24940571 ...,  0.38926506  0.2130433\n",
      "    0.17879866]\n",
      "  [ 0.28920043  0.42325446  0.21359105 ...,  0.31829548  0.14973123\n",
      "    0.16232601]\n",
      "  [ 0.29752517  0.42939359  0.20795    ...,  0.31512564  0.14879584\n",
      "    0.17110227]\n",
      "  ..., \n",
      "  [ 0.27965203  0.42742833  0.22567476 ...,  0.346596    0.15238792\n",
      "    0.17094409]\n",
      "  [ 0.29769447  0.44846344  0.23349796 ...,  0.32177445  0.14911108\n",
      "    0.17451443]\n",
      "  [ 0.28030255  0.41945767  0.21863532 ...,  0.33593351  0.15881963\n",
      "    0.17205794]]\n",
      "\n",
      " [[ 0.26826602  0.41896152  0.25255725 ...,  0.40202808  0.22649625\n",
      "    0.17460802]\n",
      "  [ 0.28659725  0.42185968  0.21742326 ...,  0.32310742  0.14460076\n",
      "    0.15697755]\n",
      "  [ 0.29496884  0.43350798  0.22162235 ...,  0.31405047  0.15166795\n",
      "    0.17260441]\n",
      "  ..., \n",
      "  [ 0.28746858  0.42644909  0.2258157  ...,  0.33405638  0.15864219\n",
      "    0.17001508]\n",
      "  [ 0.28681937  0.42704281  0.21777512 ...,  0.32991403  0.15614179\n",
      "    0.16510423]\n",
      "  [ 0.29126173  0.44038996  0.20399359 ...,  0.31280059  0.15645824\n",
      "    0.17189933]]\n",
      "\n",
      " [[ 0.26892558  0.43076679  0.25624177 ...,  0.38972312  0.20266774\n",
      "    0.174614  ]\n",
      "  [ 0.28994209  0.40923923  0.21460874 ...,  0.33788878  0.16998443\n",
      "    0.16806558]\n",
      "  [ 0.29165471  0.42134473  0.21626699 ...,  0.31837422  0.15633741\n",
      "    0.16630819]\n",
      "  ..., \n",
      "  [ 0.28684771  0.42969051  0.21613257 ...,  0.32116723  0.15346423\n",
      "    0.16323204]\n",
      "  [ 0.30609518  0.42825192  0.21615092 ...,  0.323475    0.1650089\n",
      "    0.17177486]\n",
      "  [ 0.2846019   0.42817238  0.22081617 ...,  0.31881854  0.15591213\n",
      "    0.18426377]]]\n",
      "[[ -1.53100351e-03  -4.65688342e-03  -1.11771701e-03 ...,  -1.56277046e-01\n",
      "    1.39077125e+06   1.65789288e+06]\n",
      " [  2.53393198e-04  -4.55669267e-03  -3.99578502e-03 ...,  -2.03825250e-01\n",
      "    1.28956475e+06   1.61759275e+06]\n",
      " [ -1.13332807e-03  -3.95227782e-03  -3.10186343e-03 ...,  -1.83669448e-01\n",
      "    1.27472650e+06   1.63488750e+06]\n",
      " ..., \n",
      " [ -8.29534722e-04  -3.88357625e-03  -4.27305466e-03 ...,  -2.08677560e-01\n",
      "    1.27031525e+06   1.60939775e+06]\n",
      " [  5.73607045e-04  -3.94615857e-03  -4.27166419e-03 ...,  -2.06937909e-01\n",
      "    1.29064762e+06   1.62820475e+06]\n",
      " [ -9.93254711e-04  -3.94961890e-03  -3.91821982e-03 ...,  -2.10448042e-01\n",
      "    1.27462650e+06   1.65569900e+06]]\n"
     ]
    }
   ],
   "source": [
    "testY = testY.reshape(len(testData),output_dim)\n",
    "testY = scalerOutput.inverse_transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "#create the result.cs.gzip\n",
    "content = \"Id,Predicted\\n\"\n",
    "for y in range(testY.shape[0]):\n",
    "    for x in range(testY.shape[1]-2):\n",
    "       content +=str(y+1)+\"_\"+str(x+1)+\",\"+str(testY[y][x])+\"\\n\"\n",
    "\n",
    "with gzip.open('./result.csv.gz', 'wb') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
